{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Differences in Machine Learning\n",
    "\n",
    "Information theory is used extensively in machine learning. The two most popular examples are in Cross Entropy and KL Divergence. This notebook introduces these two concepts and attempts to explain when each should be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from math import log2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KL Divergence\n",
    "\n",
    "[Kullback-Liebler (KL) Divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence), also known as relative entropy, is a statistical distance measurement of how one probability distribution, $Q$ a model, differs from a second distribution, $P$ the data. Note: while KL Divergence is a distance it is not a metric.\n",
    "\n",
    "The Kullbackâ€“Leibler divergence is then interpreted as the average difference of the number of bits required for encoding samples of $P$ using a code optimized for $Q$ rather than one optimized for $P$. Once again, for more information check out this article from [machine learning mastery](https://machinelearningmastery.com/divergence-between-probability-distributions/) that this tutorial is based on.\n",
    "\n",
    "KL Divergence Follows the equation:\n",
    "\n",
    "$$\n",
    "\\text{KL}(P || Q) = -\\sum_{x \\in X} P(x) * \\log (\\frac{Q(x)}{P(x)})\n",
    "$$\n",
    "\n",
    "The key is that the value within the sum, $P(x) * \\log (\\frac{Q(x)}{P(x)})$ is the divergence for the event x. The negative sum can be removed by simply reformatting the equation to the more common implementation:\n",
    "\n",
    "$$\n",
    "\\text{KL}(P || Q) = \\sum_{x \\in X} P(x) * \\log (\\frac{P(x)}{Q(x)})\n",
    "$$\n",
    "\n",
    "The intuition for the KL divergence score is that when the probability for an event from P is large, but the probability for the same event in Q is small, there is a large divergence. When the probability from P is small and the probability from Q is large, there is also a large divergence, but not as large as the first case.\n",
    "\n",
    "**IMPORTANT**: KL Divergence is not symmetrical, that is: $\\text{KL}(P || Q) != \\text{KL}(Q || P)$, hence why it cannot be considered a metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define distributions\n",
    "events = ['red', 'green', 'blue']\n",
    "p = [0.10, 0.40, 0.50]\n",
    "q = [0.80, 0.15, 0.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL(P || Q): 1.927 bits\n",
      "KL(Q || P): 2.022 bits\n"
     ]
    }
   ],
   "source": [
    "# calculate the kl divergence\n",
    "def kl_divergence(p, q):\n",
    "\treturn sum(p[i] * log2(p[i]/q[i]) for i in range(len(p)))\n",
    "\n",
    "# calculate (P || Q)\n",
    "kl_pq = kl_divergence(p, q)\n",
    "print('KL(P || Q): %.3f bits' % kl_pq)\n",
    "# calculate (Q || P)\n",
    "kl_qp = kl_divergence(q, p)\n",
    "print('KL(Q || P): %.3f bits' % kl_qp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Entropy\n",
    "\n",
    "Cross-entropy is a very common machine learning loss function, used extensively in classification problems. It is an extension on the concept of entropy and closely related to KL Divergence. However, where KL Divergence calculates relative entropy between distributions, cross-entropy calculates the total entropy between distributions. \n",
    "\n",
    "Note: While cross-entropy is used interchangeably with logistic loss in machine learning they are derived from completely different sources.\n",
    "\n",
    "Cross entropy follows the equation:\n",
    "\n",
    "$$\n",
    "H(P,Q) = -\\sum_{x \\in X} P(x) * \\log(Q(x))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Entropy vs KL Divergence\n",
    "\n",
    "While they are both very similar, the difference between cross-entropy and KL Divergence can be broken down into a simple issue:\n",
    "- Cross-Entropy: Average number of total bits to represent an event from Q instead of P.\n",
    "- Relative Entropy (KL Divergence): Average number of extra bits to represent an event from Q instead of P.\n",
    "\n",
    "In other words, cross entropy is the total bits and thus can follow the calculation: \n",
    "$$\n",
    "H(P, Q) = H(P) + KL(P || Q)\n",
    "$$\n",
    "\n",
    "**IMPORTANT**: Like KL Divergence, cross-entropy is not symmetrical. That is: $H(P,Q) != H(Q,P)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H(P, Q): 3.288 bits\n",
      "H(Q, P): 2.906 bits\n"
     ]
    }
   ],
   "source": [
    "def cross_entropy(p, q):\n",
    "\treturn -sum([p[i]*log2(q[i]) for i in range(len(p))])\n",
    "\n",
    "# calculate cross entropy H(P, Q)\n",
    "ce_pq = cross_entropy(p, q)\n",
    "print('H(P, Q): %.3f bits' % ce_pq)\n",
    "# calculate cross entropy H(Q, P)\n",
    "ce_qp = cross_entropy(q, p)\n",
    "print('H(Q, P): %.3f bits' % ce_qp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}